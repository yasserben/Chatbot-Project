{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can also run the notebook in [COLAB](https://colab.research.google.com/github/deepmipt/DeepPavlov/blob/master/examples/tutorials/02_deeppavlov_ner.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: deeppavlov in c:\\users\\med yasser\\appdata\\roaming\\python\\python36\\site-packages (0.1.6)\n",
      "Requirement already satisfied: fuzzywuzzy==0.16.0 in c:\\users\\med yasser\\appdata\\roaming\\python\\python36\\site-packages (from deeppavlov) (0.16.0)\n",
      "Requirement already satisfied: requests==2.19.1 in c:\\users\\med yasser\\appdata\\roaming\\python\\python36\\site-packages (from deeppavlov) (2.19.1)\n",
      "Requirement already satisfied: rusenttokenize==0.0.4 in c:\\users\\med yasser\\appdata\\roaming\\python\\python36\\site-packages (from deeppavlov) (0.0.4)\n",
      "Requirement already satisfied: scipy==1.1.0 in c:\\users\\med yasser\\appdata\\roaming\\python\\python36\\site-packages (from deeppavlov) (1.1.0)\n",
      "Requirement already satisfied: pandas==0.23.1 in c:\\users\\med yasser\\appdata\\roaming\\python\\python36\\site-packages (from deeppavlov) (0.23.1)\n",
      "Requirement already satisfied: flasgger==0.9.1 in c:\\users\\med yasser\\appdata\\roaming\\python\\python36\\site-packages (from deeppavlov) (0.9.1)\n",
      "Requirement already satisfied: pymorphy2-dicts-ru in c:\\users\\public\\mooc\\lib\\site-packages (from deeppavlov) (2.4.404381.4453942)\n",
      "Requirement already satisfied: nltk==3.2.5 in c:\\users\\public\\mooc\\lib\\site-packages (from deeppavlov) (3.2.5)\n",
      "Collecting h5py==2.8.0 (from deeppavlov)\n",
      "  Using cached https://files.pythonhosted.org/packages/12/6c/00c38c5ce9322f1cc421d93217c44739646a106c61859622eccc297a5c05/h5py-2.8.0-cp36-cp36m-win_amd64.whl\n",
      "Requirement already satisfied: flask==1.0.2 in c:\\users\\med yasser\\appdata\\roaming\\python\\python36\\site-packages (from deeppavlov) (1.0.2)\n",
      "Requirement already satisfied: tqdm==4.23.4 in c:\\users\\public\\mooc\\lib\\site-packages (from deeppavlov) (4.23.4)\n",
      "Requirement already satisfied: flask-cors==3.0.6 in c:\\users\\med yasser\\appdata\\roaming\\python\\python36\\site-packages (from deeppavlov) (3.0.6)\n",
      "Requirement already satisfied: Cython==0.28.5 in c:\\users\\med yasser\\appdata\\roaming\\python\\python36\\site-packages (from deeppavlov) (0.28.5)\n",
      "Requirement already satisfied: pymorphy2==0.8 in c:\\users\\public\\mooc\\lib\\site-packages (from deeppavlov) (0.8)\n",
      "Requirement already satisfied: pytelegrambotapi==3.5.2 in c:\\users\\med yasser\\appdata\\roaming\\python\\python36\\site-packages (from deeppavlov) (3.5.2)\n",
      "Requirement already satisfied: pyopenssl==18.0.0 in c:\\users\\med yasser\\appdata\\roaming\\python\\python36\\site-packages (from deeppavlov) (18.0.0)\n",
      "Requirement already satisfied: overrides==1.9 in c:\\users\\med yasser\\appdata\\roaming\\python\\python36\\site-packages (from deeppavlov) (1.9)\n",
      "Collecting numpy==1.14.5 (from deeppavlov)\n",
      "  Using cached https://files.pythonhosted.org/packages/0d/b7/0c804e0bcba6505f8392d042d5e333a5e06f308e019517111fbc7767a0bc/numpy-1.14.5-cp36-none-win_amd64.whl\n",
      "Requirement already satisfied: scikit-learn==0.19.1 in c:\\users\\public\\mooc\\lib\\site-packages (from deeppavlov) (0.19.1)\n",
      "Requirement already satisfied: keras==2.2.0 in c:\\users\\med yasser\\appdata\\roaming\\python\\python36\\site-packages (from deeppavlov) (2.2.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\public\\mooc\\lib\\site-packages (from requests==2.19.1->deeppavlov) (3.0.4)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in c:\\users\\public\\mooc\\lib\\site-packages (from requests==2.19.1->deeppavlov) (2.6)\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in c:\\users\\public\\mooc\\lib\\site-packages (from requests==2.19.1->deeppavlov) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\public\\mooc\\lib\\site-packages (from requests==2.19.1->deeppavlov) (2018.11.29)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in c:\\users\\public\\mooc\\lib\\site-packages (from pandas==0.23.1->deeppavlov) (2.6.1)\n",
      "Requirement already satisfied: pytz>=2011k in c:\\users\\public\\mooc\\lib\\site-packages (from pandas==0.23.1->deeppavlov) (2017.3)\n",
      "Requirement already satisfied: jsonschema>=2.5.1 in c:\\users\\public\\mooc\\lib\\site-packages (from flasgger==0.9.1->deeppavlov) (2.6.0)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\med yasser\\appdata\\roaming\\python\\python36\\site-packages (from flasgger==0.9.1->deeppavlov) (1.12.0)\n",
      "Requirement already satisfied: mistune in c:\\users\\public\\mooc\\lib\\site-packages (from flasgger==0.9.1->deeppavlov) (0.8.3)\n",
      "Requirement already satisfied: PyYAML>=3.0 in c:\\users\\public\\mooc\\lib\\site-packages (from flasgger==0.9.1->deeppavlov) (3.12)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in c:\\users\\public\\mooc\\lib\\site-packages (from flask==1.0.2->deeppavlov) (0.24)\n",
      "Requirement already satisfied: click>=5.1 in c:\\users\\public\\mooc\\lib\\site-packages (from flask==1.0.2->deeppavlov) (6.7)\n",
      "Requirement already satisfied: Werkzeug>=0.14 in c:\\users\\med yasser\\appdata\\roaming\\python\\python36\\site-packages (from flask==1.0.2->deeppavlov) (0.14.1)\n",
      "Requirement already satisfied: Jinja2>=2.10 in c:\\users\\public\\mooc\\lib\\site-packages (from flask==1.0.2->deeppavlov) (2.10)\n",
      "Requirement already satisfied: docopt>=0.6 in c:\\users\\public\\mooc\\lib\\site-packages (from pymorphy2==0.8->deeppavlov) (0.6.2)\n",
      "Requirement already satisfied: dawg-python>=0.7 in c:\\users\\public\\mooc\\lib\\site-packages (from pymorphy2==0.8->deeppavlov) (0.7.2)\n",
      "Requirement already satisfied: pymorphy2-dicts<3.0,>=2.4 in c:\\users\\public\\mooc\\lib\\site-packages (from pymorphy2==0.8->deeppavlov) (2.4.393442.3710985)\n",
      "Requirement already satisfied: cryptography>=2.2.1 in c:\\users\\public\\mooc\\lib\\site-packages (from pyopenssl==18.0.0->deeppavlov) (2.6.1)\n",
      "Collecting keras-applications==1.0.2 (from keras==2.2.0->deeppavlov)\n",
      "  Using cached https://files.pythonhosted.org/packages/e2/60/c557075e586e968d7a9c314aa38c236b37cb3ee6b37e8d57152b1a5e0b47/Keras_Applications-1.0.2-py2.py3-none-any.whl\n",
      "Collecting keras-preprocessing==1.0.1 (from keras==2.2.0->deeppavlov)\n",
      "  Using cached https://files.pythonhosted.org/packages/f8/33/275506afe1d96b221f66f95adba94d1b73f6b6087cfb6132a5655b6fe338/Keras_Preprocessing-1.0.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\public\\mooc\\lib\\site-packages (from Jinja2>=2.10->flask==1.0.2->deeppavlov) (1.0)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in c:\\users\\public\\mooc\\lib\\site-packages (from cryptography>=2.2.1->pyopenssl==18.0.0->deeppavlov) (1.11.4)\n",
      "Requirement already satisfied: asn1crypto>=0.21.0 in c:\\users\\public\\mooc\\lib\\site-packages (from cryptography>=2.2.1->pyopenssl==18.0.0->deeppavlov) (0.24.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\public\\mooc\\lib\\site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.2.1->pyopenssl==18.0.0->deeppavlov) (2.18)\n",
      "Installing collected packages: numpy, h5py, keras-applications, keras-preprocessing\n",
      "  Found existing installation: numpy 1.16.2\n",
      "    Uninstalling numpy-1.16.2:\n",
      "      Successfully uninstalled numpy-1.16.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not install packages due to an EnvironmentError: [WinError 5] Accès refusé: 'c:\\\\users\\\\med yasser\\\\appdata\\\\roaming\\\\python\\\\python36\\\\site-packages\\\\~umpy\\\\.libs\\\\libopenblas.IPBC74C7KURV7CB2PKT5Z5FNR3SIBV4J.gfortran-win_amd64.dll'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip3 install deeppavlov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognize named entities on news data with CNN\n",
    "\n",
    "In this tutorial, you will use a convolutional neural network to solve Named Entity Recognition (NER) problem. NER is a common task in natural language processing systems. It serves for extraction such entities from the text as persons, organizations, locations, etc. In this task you will experiment to recognize named entities in different news from common CoNLL-2003 dataset.\n",
    "\n",
    "For example, we want to extract persons' and organizations' names from the text. Then for the input text:\n",
    "\n",
    "    Yan Goodfellow works for Google Brain\n",
    "\n",
    "a NER model needs to provide the following sequence of tags:\n",
    "\n",
    "    B-PER I-PER    O     O   B-ORG  I-ORG\n",
    "\n",
    "Where *B-* and *I-* prefixes stand for the beginning and inside of the entity, while *O* stands for out of tag or no tag. Markup with the prefix scheme is called *BIO markup*. This markup is introduced for distinguishing of consequent entities with similar types.\n",
    "\n",
    "A solution of the task will be based on neural networks, particularly, on Convolutional Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The following cell will download all data required for this assignment into the folder `/data`. The download util from the library is used to download and extract the archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-18 14:00:39.507 DEBUG in 'urllib3.connectionpool'['connectionpool'] at line 208: Starting new HTTP connection (1): files.deeppavlov.ai\n",
      "2019-03-18 14:00:39.658 DEBUG in 'urllib3.connectionpool'['connectionpool'] at line 396: http://files.deeppavlov.ai:80 \"GET /deeppavlov_data/conll2003_v2.tar.gz HTTP/1.1\" 200 957092\n",
      "2019-03-18 14:00:39.661 INFO in 'deeppavlov.core.data.utils'['utils'] at line 64: Downloading from http://files.deeppavlov.ai/deeppavlov_data/conll2003_v2.tar.gz to data\\conll2003_v2.tar.gz\n",
      "100%|██████████| 957k/957k [00:01<00:00, 881kB/s] \n",
      "2019-03-18 14:00:40.914 INFO in 'deeppavlov.core.data.utils'['utils'] at line 202: Extracting data\\conll2003_v2.tar.gz archive into data\n"
     ]
    }
   ],
   "source": [
    "import deeppavlov\n",
    "from deeppavlov.core.data.utils import download_decompress\n",
    "download_decompress('http://files.deeppavlov.ai/deeppavlov_data/conll2003_v2.tar.gz', 'data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the CoNLL-2003 Named Entity Recognition corpus\n",
    "\n",
    "We will work with a corpus, which contains twits with NE tags. Typical file with NER data contains lines with pairs of tokens (word/punctuation symbol) and tags, separated by a whitespace. In many cases additional information such as POS tags included between  Different documents are separated by lines **started** with **-DOCSTART-** token. Different sentences are separated by an empty line. Example\n",
    "\n",
    "    -DOCSTART- -X- -X- O\n",
    "\n",
    "    EU NNP B-NP B-ORG\n",
    "    rejects VBZ B-VP O\n",
    "    German JJ B-NP B-MISC\n",
    "    call NN I-NP O\n",
    "    to TO B-VP O\n",
    "    boycott VB I-VP O\n",
    "    British JJ B-NP B-MISC\n",
    "    lamb NN I-NP O\n",
    "    . . O O\n",
    "\n",
    "    Peter NNP B-NP B-PER\n",
    "    Blackburn NNP I-NP I-PER\n",
    "\n",
    "In this tutorial we will focus only on tokens and tags (first and last elements of the line) and drop POS information located in between.\n",
    "\n",
    "We start with using the *Conll2003DatasetReader* class that provides functionality for reading the dataset. It returns a dictionary with fields *train*, *test*, and *valid*. At each field a list of samples is stored. Each sample is a tuple of tokens and tags. Both tokens and tags are lists. The following example depicts the structure that should be returned by *read* method:\n",
    "\n",
    "    {'train': [(['Mr.', 'Dwag', 'are', 'derping', 'around'], ['B-PER', 'I-PER', 'O', 'O', 'O']), ....],\n",
    "     'valid': [...],\n",
    "     'test': [...]}\n",
    "\n",
    "There are three separate parts of the dataset:\n",
    " - *train* data for training the model;\n",
    " - *validation* data for evaluation and hyperparameters tuning;\n",
    " - *test* data for final evaluation of the model.\n",
    " \n",
    "\n",
    "Each of these parts is stored in a separate txt file.\n",
    "\n",
    "We will use [Conll2003DatasetReader](https://github.com/deepmipt/DeepPavlov/blob/master/deeppavlov/dataset_readers/conll2003_reader.py) from the library to read the data from text files to the format described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.dataset_readers.conll2003_reader import Conll2003DatasetReader\n",
    "dataset = Conll2003DatasetReader().read('data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should always understand what kind of data you deal with. For this purpose, you can print the data running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DOCSTART>\tO\n",
      "\n",
      "EU\tB-ORG\n",
      "rejects\tO\n",
      "German\tB-MISC\n",
      "call\tO\n",
      "to\tO\n",
      "boycott\tO\n",
      "British\tB-MISC\n",
      "lamb\tO\n",
      ".\tO\n",
      "\n",
      "Peter\tB-PER\n",
      "Blackburn\tI-PER\n",
      "\n",
      "BRUSSELS\tB-LOC\n",
      "1996-08-22\tO\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sample in dataset['train'][:4]:\n",
    "    for token, tag in zip(*sample):\n",
    "        print('%s\\t%s' % (token, tag))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dictionaries\n",
    "\n",
    "To train a neural network, we will use two mappings: \n",
    "- {token}$\\to${token id}: address the row in embeddings matrix for the current token;\n",
    "- {tag}$\\to${tag id}: one-hot ground truth probability distribution vectors for computing the loss at the output of the network.\n",
    "\n",
    "Token indices will be used to address the row in embeddings matrix. The mapping for tags will be used to create one-hot ground truth probability distribution vectors to compute the loss at the output of the network.\n",
    "\n",
    "The [SimpleVocabulary](https://github.com/deepmipt/DeepPavlov/blob/master/deeppavlov/core/data/simple_vocab.py) implemented in the library will be used to perform those mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.core.data.simple_vocab import SimpleVocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to build dictionaries for tokens and tags. Sometimes there are special tokens in vocabularies, for instance an unknown word token, which is used every time we encounter out of vocabulary word. In our case the only special token will be`<UNK>` for out of vocabulary words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-18 14:03:32.558 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 48: No load path is set for SimpleVocabulary in 'infer' mode. Using save path instead\n",
      "2019-03-18 14:03:32.562 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 48: No load path is set for SimpleVocabulary in 'infer' mode. Using save path instead\n"
     ]
    }
   ],
   "source": [
    "special_tokens = ['<UNK>']\n",
    "\n",
    "token_vocab = SimpleVocabulary(special_tokens, save_path='model/token.dict')\n",
    "tag_vocab = SimpleVocabulary(save_path='model/tag.dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets fit the vocabularies on the train part of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens_by_sentences = [tokens for tokens, tags in dataset['train']]\n",
    "all_tags_by_sentences = [tags for tokens, tags in dataset['train']]\n",
    "\n",
    "token_vocab.fit(all_tokens_by_sentences)\n",
    "tag_vocab.fit(all_tags_by_sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to get the indices. Keep in mind that we are working with batches of the following structure:\n",
    "    \n",
    "    [['utt0_tok0', 'utt1_tok1', ...], ['utt1_tok0', 'utt1_tok1', ...], ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10167, 6, 168, 7, 6097, 5518, 1865]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_vocab([['How', 'to', 'do', 'a', 'barrel', 'roll', '?']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0], [3, 5]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_vocab([['O', 'O', 'O'], ['B-ORG', 'I-ORG']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try conversion from indices to tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Men',\n",
       "  'lower',\n",
       "  'lead',\n",
       "  'end',\n",
       "  'president',\n",
       "  '1996-08-23',\n",
       "  'b',\n",
       "  'news',\n",
       "  'would',\n",
       "  'southern']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "token_vocab([np.random.randint(0, 512, size=10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Iterator\n",
    "\n",
    "Neural Networks are usually trained with batches. It means that weight updates of the network are based on several sequences at every single time. The tricky part is that all sequences within a batch need to have the same length. So we will pad them with a special `<UNK>` token. Likewise tokens tags also must be padded It is also a good practice to provide RNN with sequence lengths, so it can skip computations for padding parts. We provide the batching function *batches_generator* readily available for you to save time. \n",
    "\n",
    "An important concept in the batch generation is shuffling. Shuffling is taking sample from the dataset at random order. It is important to train on the shuffled data because large number consequetive samples of the same class may result in pure quality of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.core.data.data_learning_iterator import DataLearningIterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dataset iterator from the loaded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iterator = DataLearningIterator(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((['CHICAGO', '63', '63', '.500', '4'], ['I', \"'m\", 'only', '34', '.']),\n",
       " (['B-ORG', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O']))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(data_iterator.gen_batches(2, shuffle=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing about generating training data. We need to produce a binary mask which is one where tokens present and zero elsewhere. This mask will stop backpropagation through paddings. An instance of such mask:\n",
    "\n",
    "    [[1, 1, 0, 0, 0],\n",
    "     [1, 1, 1, 1, 1]]\n",
    " For the sentences in batch:\n",
    "\n",
    "     [['The', 'roof'],\n",
    "      ['This', 'is', 'my', 'domain', '!']]\n",
    "\n",
    "The mask length must be equal to the maximum length of the sentence in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Med\n",
      "[nltk_data]     Yasser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Med\n",
      "[nltk_data]     Yasser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package perluniprops to C:\\Users\\Med\n",
      "[nltk_data]     Yasser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping misc\\perluniprops.zip.\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to C:\\Users\\Med\n",
      "[nltk_data]     Yasser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\nonbreaking_prefixes.zip.\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov.models.preprocessors.mask import Mask\n",
    "get_mask = Mask()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1.],\n",
       "       [1., 1., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mask([['Try', 'to', 'get', 'the', 'mask'], ['Check', 'paddings']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a recurrent neural network\n",
    "\n",
    "This is the most important part of the assignment. Here we will specify the network architecture based on TensorFlow building blocks. It's fun and easy as a lego constructor! We will create an Convolutional Neural Network (CNN) network which will produce probability distribution over tags for each token in a sentence. To take into account both right and left contexts of the token, we will use CNN. Dense layer will be used on top to perform tag classification.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An essential part of almost every network in NLP domain is embeddings of the words. We pass the text to the network as a series of tokens. Each token is represented by its index. For every token (index) we have a vector. In total the vectors form an embedding matrix. This matrix can be either pretrained using some common algorithm like Skip-Gram or CBOW or it can be initialized by random values and trained along with other parameters of the network. In this tutorial we will follow the second alternative.\n",
    "\n",
    "We need to build a function that takes the tensor of token indices with shape [batch_size, num_tokens] and for each index in this matrix it retrieves a vector from the embedding matrix, corresponding to that index. That results in a new tensor with sahpe [batch_size, num_tokens, emb_dim]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(indices, vocabulary_size, emb_dim):\n",
    "    # Initialize the random gaussian matrix with dimensions [vocabulary_size, embedding_dimension]\n",
    "    # The **VARIANCE** of the random samples must be 1 / embedding_dimension\n",
    "    emb_mat = np.random.randn(vocabulary_size, emb_dim).astype(np.float32) / np.sqrt(emb_dim) # YOUR CODE HERE\n",
    "    emb_mat = tf.Variable(emb_mat, name='Embeddings', trainable=True)\n",
    "    emb = tf.nn.embedding_lookup(emb_mat, indices)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The body of the network is the convolutional layers. The basic idea behind convolutions is to apply the same dense layer to every n consecutive samples (tokens in our case). A simplified case is depicted below.\n",
    "\n",
    "<img src=\"img/convolution.png\" width=\"400\">\n",
    "\n",
    "Here number of input and output features equal to 1.\n",
    "\n",
    "Lets try it on a toy example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"conv1d_6/BiasAdd:0\", shape=(2, 3, 200), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor with shape [batch_size, number_of_tokens, number_of_features]\n",
    "x = tf.random_normal(shape=[2, 10, 100])\n",
    "y = tf.layers.conv1d(x, filters=200, kernel_size=8)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see due to the abscence of zero padding (zeros on in the beginning and in the end of input) the size of resulting tensor along the token dimension is reduced. To use padding and preserve the dimensionality along the convolution dimension pass padding='same' parameter to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"conv1d_7/BiasAdd:0\", shape=(2, 10, 200), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "y_with_padding = tf.layers.conv1d(x, filters=200, kernel_size=8, padding='same')\n",
    "print(y_with_padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now stack a number of layers with dimensionality given in n_hidden_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_net(units, n_hidden_list, cnn_filter_width, activation=tf.nn.relu):\n",
    "    # Use activation(units) to apply activation to units\n",
    "    for n_hidden in n_hidden_list:\n",
    "        \n",
    "        units = tf.layers.conv1d(units,\n",
    "                                 n_hidden,\n",
    "                                 cnn_filter_width,\n",
    "                                 padding='same')\n",
    "        units = activation(units)\n",
    "    return units\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common loss for the classification task is cross-entropy. Why classification? Because for each token the network must decide which tag to predict. The cross-entropy has the following form:\n",
    "\n",
    "$$ H(P, Q) = -E_{x \\sim P} log Q(x) $$\n",
    "\n",
    "It measures the dissimilarity between the ground truth distribution over the classes and predicted distribution. In the most of the cases ground truth distribution is one-hot. Luckily this loss is already [implemented](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits_v2) in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"softmax_cross_entropy_with_logits_3/Reshape_2:0\", shape=(1, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# The logits\n",
    "l = tf.random_normal([1, 4, 3]) # shape [batch_size, number_of_tokens, number of classes]\n",
    "indices = tf.placeholder(tf.int32, [1, 4])\n",
    "\n",
    "# Make one-hot distribution from indices for 3 types of tag\n",
    "p = tf.one_hot(indices, depth=3)\n",
    "loss_tensor = tf.nn.softmax_cross_entropy_with_logits_v2(labels=p, logits=l)\n",
    "print(loss_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All sentences in the batch have same length and we pad the each sentence to the maximal lendth. So there are paddings at the end and pushing the network to predict those paddings usually results in deteriorated quallity. Then we need to multiply the loss tensor by binary mask to prevent gradient flow from the paddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = tf.placeholder(tf.float32, shape=[1, 4])\n",
    "loss_tensor *= mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step to do is to compute the mean value of the loss tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(loss_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define your own function that returns a scalar masked cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_cross_entropy(logits, label_indices, number_of_tags, mask):\n",
    "    ground_truth_labels = tf.one_hot(label_indices, depth=number_of_tags)\n",
    "    loss_tensor = tf.nn.softmax_cross_entropy_with_logits_v2(labels=ground_truth_labels, logits=logits)\n",
    "    loss_tensor *= mask\n",
    "    loss = tf.reduce_mean(loss_tensor)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put everything into a class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class NerNetwork:\n",
    "    def __init__(self,\n",
    "                 n_tokens,\n",
    "                 n_tags,\n",
    "                 token_emb_dim=100,\n",
    "                 n_hidden_list=(128,),\n",
    "                 cnn_filter_width=7,\n",
    "                 use_batch_norm=False,\n",
    "                 embeddings_dropout=False,\n",
    "                 top_dropout=False,\n",
    "                 **kwargs):\n",
    "        \n",
    "        # ================ Building inputs =================\n",
    "        \n",
    "        self.learning_rate_ph = tf.placeholder(tf.float32, [])\n",
    "        self.dropout_keep_ph = tf.placeholder(tf.float32, [])\n",
    "        self.token_ph = tf.placeholder(tf.int32, [None, None], name='token_ind_ph')\n",
    "        self.mask_ph = tf.placeholder(tf.float32, [None, None], name='Mask_ph')\n",
    "        self.y_ph = tf.placeholder(tf.int32, [None, None], name='y_ph')\n",
    "        \n",
    "        # ================== Building the network ==================\n",
    "        \n",
    "        # Now embedd the indices of tokens using token_emb_dim function\n",
    "        \n",
    "        ######################################\n",
    "        ########## YOUR CODE HERE ############\n",
    "        emb = get_embeddings(self.token_ph, n_tokens, token_emb_dim)\n",
    "        ######################################\n",
    "\n",
    "        emb = tf.nn.dropout(emb, self.dropout_keep_ph, (tf.shape(emb)[0], 1, tf.shape(emb)[2]))\n",
    "        \n",
    "        # Build a multilayer CNN on top of the embeddings.\n",
    "        # The number of units in the each layer must match\n",
    "        # corresponding number from n_hidden_list.\n",
    "        # Use ReLU activation \n",
    "        ######################################\n",
    "        ########## YOUR CODE HERE ############\n",
    "        units = conv_net(emb, n_hidden_list, cnn_filter_width)\n",
    "        ######################################\n",
    "        units = tf.nn.dropout(units, self.dropout_keep_ph, (tf.shape(units)[0], 1, tf.shape(units)[2]))\n",
    "        logits = tf.layers.dense(units, n_tags, activation=None)\n",
    "        self.predictions = tf.argmax(logits, 2)\n",
    "        \n",
    "        # ================= Loss and train ops =================\n",
    "        # Use cross-entropy loss. check the tf.nn.softmax_cross_entropy_with_logits_v2 function\n",
    "        ######################################\n",
    "        ########## YOUR CODE HERE ############\n",
    "        self.loss = masked_cross_entropy(logits, self.y_ph, n_tags, self.mask_ph)\n",
    "        ######################################\n",
    "\n",
    "        # Create a training operation to update the network parameters.\n",
    "        # We purpose to use the Adam optimizer as it work fine for the\n",
    "        # most of the cases. Check tf.train to find an implementation.\n",
    "        # Put the train operation to the attribute self.train_op\n",
    "        \n",
    "        ######################################\n",
    "        ########## YOUR CODE HERE ############\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate_ph)\n",
    "        self.train_op = optimizer.minimize(self.loss)\n",
    "        ######################################\n",
    "\n",
    "        # ================= Initialize the session =================\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def __call__(self, tok_batch, mask_batch):\n",
    "        feed_dict = {self.token_ph: tok_batch,\n",
    "                     self.mask_ph: mask_batch,\n",
    "                     self.dropout_keep_ph: 1.0}\n",
    "        return self.sess.run(self.predictions, feed_dict)\n",
    "\n",
    "    def train_on_batch(self, tok_batch, tag_batch, mask_batch, dropout_keep_prob, learning_rate):\n",
    "        feed_dict = {self.token_ph: tok_batch,\n",
    "                     self.y_ph: tag_batch,\n",
    "                     self.mask_ph: mask_batch,\n",
    "                     self.dropout_keep_ph: dropout_keep_prob,\n",
    "                     self.learning_rate_ph: learning_rate}\n",
    "        self.sess.run(self.train_op, feed_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create an instance of the NerNetwork class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "nernet = NerNetwork(len(token_vocab),\n",
    "                    len(tag_vocab),\n",
    "                    n_hidden_list=[100, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularly we want to check the score on validation part of the dataset every epoch. In the most of the cases of NER tasks the classes are imbalanced. And the accuray is not the best measure of performance. If we have 95% of 'O' tags, than the silly classifier, that always predicts '0' get 95% accuracy. To tackle this issue the F1-score is used. The F1-score can be defined as:\n",
    "\n",
    "$$ F1 =  \\frac{2 P R}{P + R}$$ \n",
    "\n",
    "where P is precision and R is recall.\n",
    "\n",
    "Lets write the evaluation function. We need to get all predictions for the given part of the dataset and compute F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.models.ner.evaluation import precision_recall_f1\n",
    "# The function precision_recall_f1 takes two lists: y_true and y_predicted\n",
    "# the tag sequences for each sentences should be merged into one big list \n",
    "from deeppavlov.core.data.utils import zero_pad\n",
    "# zero_pad takes a batch of lists of token indices, pad it with zeros to the\n",
    "# maximal length and convert it to numpy matrix\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "def eval_valid(network, batch_generator):\n",
    "    total_true = []\n",
    "    total_pred = []\n",
    "    for x, y_true in batch_generator:\n",
    "\n",
    "        # Prepare token indices from tokens batch\n",
    "        x_inds = token_vocab(x) # YOUR CODE HERE\n",
    "\n",
    "        # Pad the indices batch with zeros\n",
    "        x_batch = zero_pad(x_inds) # YOUR CODE HERE\n",
    "\n",
    "        # Get the mask using get_mask\n",
    "        mask = get_mask(x) # YOUR CODE HERE\n",
    "        \n",
    "        # We call the instance of the NerNetwork because we have defined __call__ method\n",
    "        y_inds = network(x_batch, mask)\n",
    "\n",
    "        # For every sentence in the batch extract all tags up to paddings\n",
    "        y_inds = [y_inds[n][:len(x[n])] for n, y in enumerate(y_inds)] # YOUR CODE HERE\n",
    "        y_pred = tag_vocab(y_inds)\n",
    "\n",
    "        # Add fresh predictions \n",
    "        total_true.extend(chain(*y_true))\n",
    "        total_pred.extend(chain(*y_pred))\n",
    "    res = precision_recall_f1(total_true, total_pred, print_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set hyperparameters. You might want to start with the following recommended values:\n",
    "- *batch_size*: 32;\n",
    "- n_epochs: 10;\n",
    "- starting value of *learning_rate*: 0.001\n",
    "- *learning_rate_decay*: a square root of 2;\n",
    "- *dropout_keep_probability* equal to 0.7 for training (typical values for dropout probability are ranging from 0.3 to 0.9).\n",
    "\n",
    "A very efficient technique for the learning rate managment is dropping learning rate after convergence. It is common to use dividers 2, 3, and 10 to drop the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16 # YOUR HYPERPARAMETER HERE\n",
    "n_epochs = 20 # YOUR HYPERPARAMETER HERE\n",
    "learning_rate = 0.001 # YOUR HYPERPARAMETER HERE\n",
    "dropout_keep_prob = 0.5 # YOUR HYPERPARAMETER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we iterate through dataset batch by batch and pass the data to the train op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-27 13:45:24.341 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51362 tokens with 5942 phrases; found: 5465 phrases; correct: 3397.\n",
      "\n",
      "precision:  62.16%; recall:  57.17%; FB1:  59.56\n",
      "\n",
      "\tLOC: precision:  64.94%; recall:  76.65%; F1:  70.31 2168\n",
      "\n",
      "\tMISC: precision:  51.44%; recall:  19.41%; F1:  28.19 348\n",
      "\n",
      "\tORG: precision:  50.16%; recall:  46.61%; F1:  48.32 1246\n",
      "\n",
      "\tPER: precision:  69.58%; recall:  64.33%; F1:  66.85 1703\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-27 13:45:27.357 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51362 tokens with 5942 phrases; found: 5615 phrases; correct: 4447.\n",
      "\n",
      "precision:  79.20%; recall:  74.84%; FB1:  76.96\n",
      "\n",
      "\tLOC: precision:  86.10%; recall:  84.00%; F1:  85.04 1792\n",
      "\n",
      "\tMISC: precision:  67.88%; recall:  68.76%; F1:  68.32 934\n",
      "\n",
      "\tORG: precision:  75.43%; recall:  61.82%; F1:  67.95 1099\n",
      "\n",
      "\tPER: precision:  80.50%; recall:  78.23%; F1:  79.35 1790\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-27 13:45:30.326 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51362 tokens with 5942 phrases; found: 5387 phrases; correct: 4584.\n",
      "\n",
      "precision:  85.09%; recall:  77.15%; FB1:  80.93\n",
      "\n",
      "\tLOC: precision:  89.46%; recall:  85.90%; F1:  87.64 1764\n",
      "\n",
      "\tMISC: precision:  85.34%; recall:  73.86%; F1:  79.19 798\n",
      "\n",
      "\tORG: precision:  80.15%; recall:  70.77%; F1:  75.17 1184\n",
      "\n",
      "\tPER: precision:  83.85%; recall:  74.70%; F1:  79.01 1641\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-27 13:45:33.277 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51362 tokens with 5942 phrases; found: 5436 phrases; correct: 4702.\n",
      "\n",
      "precision:  86.50%; recall:  79.13%; FB1:  82.65\n",
      "\n",
      "\tLOC: precision:  89.44%; recall:  86.72%; F1:  88.06 1781\n",
      "\n",
      "\tMISC: precision:  88.02%; recall:  75.70%; F1:  81.40 793\n",
      "\n",
      "\tORG: precision:  83.25%; recall:  71.14%; F1:  76.72 1146\n",
      "\n",
      "\tPER: precision:  84.91%; recall:  79.10%; F1:  81.90 1716\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-27 13:45:36.266 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51362 tokens with 5942 phrases; found: 5298 phrases; correct: 4688.\n",
      "\n",
      "precision:  88.49%; recall:  78.90%; FB1:  83.42\n",
      "\n",
      "\tLOC: precision:  92.71%; recall:  85.90%; F1:  89.18 1702\n",
      "\n",
      "\tMISC: precision:  89.10%; recall:  77.98%; F1:  83.17 807\n",
      "\n",
      "\tORG: precision:  82.95%; recall:  75.09%; F1:  78.83 1214\n",
      "\n",
      "\tPER: precision:  87.87%; recall:  75.14%; F1:  81.01 1575\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-27 13:45:39.228 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51362 tokens with 5942 phrases; found: 5339 phrases; correct: 4705.\n",
      "\n",
      "precision:  88.13%; recall:  79.18%; FB1:  83.41\n",
      "\n",
      "\tLOC: precision:  90.68%; recall:  86.88%; F1:  88.74 1760\n",
      "\n",
      "\tMISC: precision:  86.87%; recall:  78.20%; F1:  82.31 830\n",
      "\n",
      "\tORG: precision:  83.80%; recall:  74.42%; F1:  78.83 1191\n",
      "\n",
      "\tPER: precision:  89.22%; recall:  75.46%; F1:  81.76 1558\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-27 13:45:42.213 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51362 tokens with 5942 phrases; found: 5412 phrases; correct: 4789.\n",
      "\n",
      "precision:  88.49%; recall:  80.60%; FB1:  84.36\n",
      "\n",
      "\tLOC: precision:  93.45%; recall:  86.94%; F1:  90.07 1709\n",
      "\n",
      "\tMISC: precision:  89.57%; recall:  79.18%; F1:  84.05 815\n",
      "\n",
      "\tORG: precision:  81.04%; recall:  76.81%; F1:  78.87 1271\n",
      "\n",
      "\tPER: precision:  88.56%; recall:  77.74%; F1:  82.80 1617\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-27 13:45:45.169 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51362 tokens with 5942 phrases; found: 5388 phrases; correct: 4763.\n",
      "\n",
      "precision:  88.40%; recall:  80.16%; FB1:  84.08\n",
      "\n",
      "\tLOC: precision:  91.84%; recall:  88.19%; F1:  89.98 1764\n",
      "\n",
      "\tMISC: precision:  87.17%; recall:  78.85%; F1:  82.80 834\n",
      "\n",
      "\tORG: precision:  82.20%; recall:  75.09%; F1:  78.49 1225\n",
      "\n",
      "\tPER: precision:  90.03%; recall:  76.49%; F1:  82.71 1565\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-27 13:45:48.170 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51362 tokens with 5942 phrases; found: 5345 phrases; correct: 4722.\n",
      "\n",
      "precision:  88.34%; recall:  79.47%; FB1:  83.67\n",
      "\n",
      "\tLOC: precision:  92.03%; recall:  87.43%; F1:  89.67 1745\n",
      "\n",
      "\tMISC: precision:  88.51%; recall:  79.39%; F1:  83.70 827\n",
      "\n",
      "\tORG: precision:  80.78%; recall:  75.84%; F1:  78.23 1259\n",
      "\n",
      "\tPER: precision:  90.29%; recall:  74.21%; F1:  81.47 1514\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-27 13:45:51.116 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51362 tokens with 5942 phrases; found: 5273 phrases; correct: 4716.\n",
      "\n",
      "precision:  89.44%; recall:  79.37%; FB1:  84.10\n",
      "\n",
      "\tLOC: precision:  92.76%; recall:  87.21%; F1:  89.90 1727\n",
      "\n",
      "\tMISC: precision:  90.83%; recall:  79.50%; F1:  84.79 807\n",
      "\n",
      "\tORG: precision:  82.51%; recall:  75.99%; F1:  79.11 1235\n",
      "\n",
      "\tPER: precision:  90.56%; recall:  73.94%; F1:  81.41 1504\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-27 13:45:54.39 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51362 tokens with 5942 phrases; found: 5183 phrases; correct: 4632.\n",
      "\n",
      "precision:  89.37%; recall:  77.95%; FB1:  83.27\n",
      "\n",
      "\tLOC: precision:  93.43%; recall:  85.90%; F1:  89.51 1689\n",
      "\n",
      "\tMISC: precision:  90.86%; recall:  79.83%; F1:  84.99 810\n",
      "\n",
      "\tORG: precision:  83.84%; recall:  75.47%; F1:  79.43 1207\n",
      "\n",
      "\tPER: precision:  88.42%; recall:  70.90%; F1:  78.70 1477\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-27 13:45:56.988 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51362 tokens with 5942 phrases; found: 5162 phrases; correct: 4587.\n",
      "\n",
      "precision:  88.86%; recall:  77.20%; FB1:  82.62\n",
      "\n",
      "\tLOC: precision:  92.82%; recall:  85.90%; F1:  89.23 1700\n",
      "\n",
      "\tMISC: precision:  90.65%; recall:  79.93%; F1:  84.96 813\n",
      "\n",
      "\tORG: precision:  82.99%; recall:  74.94%; F1:  78.76 1211\n",
      "\n",
      "\tPER: precision:  88.11%; recall:  68.78%; F1:  77.26 1438\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-27 13:45:59.925 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51362 tokens with 5942 phrases; found: 5220 phrases; correct: 4630.\n",
      "\n",
      "precision:  88.70%; recall:  77.92%; FB1:  82.96\n",
      "\n",
      "\tLOC: precision:  93.66%; recall:  86.12%; F1:  89.73 1689\n",
      "\n",
      "\tMISC: precision:  90.63%; recall:  79.72%; F1:  84.82 811\n",
      "\n",
      "\tORG: precision:  82.79%; recall:  76.06%; F1:  79.28 1232\n",
      "\n",
      "\tPER: precision:  86.90%; recall:  70.20%; F1:  77.66 1488\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-27 13:46:02.877 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51362 tokens with 5942 phrases; found: 5220 phrases; correct: 4650.\n",
      "\n",
      "precision:  89.08%; recall:  78.26%; FB1:  83.32\n",
      "\n",
      "\tLOC: precision:  93.30%; recall:  86.45%; F1:  89.74 1702\n",
      "\n",
      "\tMISC: precision:  90.98%; recall:  79.83%; F1:  85.04 809\n",
      "\n",
      "\tORG: precision:  82.74%; recall:  75.09%; F1:  78.73 1217\n",
      "\n",
      "\tPER: precision:  88.40%; recall:  71.61%; F1:  79.12 1492\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-27 13:46:05.850 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51362 tokens with 5942 phrases; found: 5220 phrases; correct: 4678.\n",
      "\n",
      "precision:  89.62%; recall:  78.73%; FB1:  83.82\n",
      "\n",
      "\tLOC: precision:  92.90%; recall:  86.94%; F1:  89.82 1719\n",
      "\n",
      "\tMISC: precision:  90.42%; recall:  79.83%; F1:  84.79 814\n",
      "\n",
      "\tORG: precision:  83.96%; recall:  74.94%; F1:  79.20 1197\n",
      "\n",
      "\tPER: precision:  89.93%; recall:  72.75%; F1:  80.43 1490\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-27 13:46:08.832 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51362 tokens with 5942 phrases; found: 5163 phrases; correct: 4599.\n",
      "\n",
      "precision:  89.08%; recall:  77.40%; FB1:  82.83\n",
      "\n",
      "\tLOC: precision:  91.82%; recall:  86.77%; F1:  89.22 1736\n",
      "\n",
      "\tMISC: precision:  91.66%; recall:  79.83%; F1:  85.33 803\n",
      "\n",
      "\tORG: precision:  85.16%; recall:  73.60%; F1:  78.96 1159\n",
      "\n",
      "\tPER: precision:  87.51%; recall:  69.60%; F1:  77.53 1465\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-27 13:46:11.711 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51362 tokens with 5942 phrases; found: 5200 phrases; correct: 4672.\n",
      "\n",
      "precision:  89.85%; recall:  78.63%; FB1:  83.86\n",
      "\n",
      "\tLOC: precision:  93.62%; recall:  86.23%; F1:  89.77 1692\n",
      "\n",
      "\tMISC: precision:  89.32%; recall:  79.83%; F1:  84.31 824\n",
      "\n",
      "\tORG: precision:  83.79%; recall:  75.17%; F1:  79.25 1203\n",
      "\n",
      "\tPER: precision:  90.75%; recall:  72.96%; F1:  80.89 1481\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-27 13:46:14.542 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51362 tokens with 5942 phrases; found: 5196 phrases; correct: 4645.\n",
      "\n",
      "precision:  89.40%; recall:  78.17%; FB1:  83.41\n",
      "\n",
      "\tLOC: precision:  93.58%; recall:  85.68%; F1:  89.46 1682\n",
      "\n",
      "\tMISC: precision:  88.96%; recall:  79.50%; F1:  83.96 824\n",
      "\n",
      "\tORG: precision:  83.78%; recall:  74.72%; F1:  78.99 1196\n",
      "\n",
      "\tPER: precision:  89.42%; recall:  72.53%; F1:  80.10 1494\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-27 13:46:17.420 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51362 tokens with 5942 phrases; found: 5140 phrases; correct: 4553.\n",
      "\n",
      "precision:  88.58%; recall:  76.62%; FB1:  82.17\n",
      "\n",
      "\tLOC: precision:  93.59%; recall:  85.85%; F1:  89.55 1685\n",
      "\n",
      "\tMISC: precision:  90.83%; recall:  79.50%; F1:  84.79 807\n",
      "\n",
      "\tORG: precision:  85.03%; recall:  74.57%; F1:  79.46 1176\n",
      "\n",
      "\tPER: precision:  84.44%; recall:  67.48%; F1:  75.02 1472\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-27 13:46:20.380 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51362 tokens with 5942 phrases; found: 5196 phrases; correct: 4666.\n",
      "\n",
      "precision:  89.80%; recall:  78.53%; FB1:  83.79\n",
      "\n",
      "\tLOC: precision:  91.30%; recall:  88.02%; F1:  89.63 1771\n",
      "\n",
      "\tMISC: precision:  92.10%; recall:  79.61%; F1:  85.40 797\n",
      "\n",
      "\tORG: precision:  85.65%; recall:  74.79%; F1:  79.86 1171\n",
      "\n",
      "\tPER: precision:  90.05%; recall:  71.23%; F1:  79.54 1457\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    for x, y in data_iterator.gen_batches(batch_size, 'train'):\n",
    "        # Convert tokens to indices via Vocab\n",
    "        x_inds = token_vocab(x) # YOUR CODE \n",
    "        # Convert tags to indices via Vocab\n",
    "        y_inds = tag_vocab(y) # YOUR CODE \n",
    "        \n",
    "        # Pad every sample with zeros to the maximal length\n",
    "        x_batch = zero_pad(x_inds)\n",
    "        y_batch = zero_pad(y_inds)\n",
    "\n",
    "        mask = get_mask(x)\n",
    "        nernet.train_on_batch(x_batch, y_batch, mask, dropout_keep_prob, learning_rate)\n",
    "    print('Evaluating the model on valid part of the dataset')\n",
    "    eval_valid(nernet, data_iterator.gen_batches(batch_size, 'valid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval the model on test part now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-27 13:46:35.397 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 46435 tokens with 5648 phrases; found: 4561 phrases; correct: 3738.\n",
      "\n",
      "precision:  81.96%; recall:  66.18%; FB1:  73.23\n",
      "\n",
      "\tLOC: precision:  84.02%; recall:  82.25%; F1:  83.13 1633\n",
      "\n",
      "\tMISC: precision:  81.80%; recall:  71.08%; F1:  76.07 610\n",
      "\n",
      "\tORG: precision:  81.25%; recall:  60.26%; F1:  69.20 1232\n",
      "\n",
      "\tPER: precision:  79.74%; recall:  53.56%; F1:  64.08 1086\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_valid(nernet, data_iterator.gen_batches(batch_size, 'test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try to infer the model on our sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Petr', 'stole', 'my', 'vodka']\n",
      "['B-PER', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Petr stole my vodka'\n",
    "x = [sentence.split()]\n",
    "\n",
    "x_inds = token_vocab(x)\n",
    "x_batch = zero_pad(x_inds)\n",
    "mask = get_mask(x)\n",
    "y_inds = nernet(x_batch, mask)\n",
    "print(x[0])\n",
    "print(tag_vocab(y_inds)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
